{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import torch.optim as optim\n",
    "from custom_models.YOLOv3.model import YOLOv3\n",
    "from tqdm import tqdm\n",
    "\n",
    "from custom_models.YOLOv3.utils import (\n",
    "    mean_average_precision,\n",
    "    get_evaluation_bboxes,\n",
    "    check_class_accuracy,\n",
    "    get_loaders,\n",
    "    cells_to_bboxes,\n",
    "    non_max_suppression,\n",
    "    intersection_over_union\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "from custom_models.YOLOv3.loss import YoloLoss\n",
    "loss_fn = YoloLoss()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, train_eval_loader,test_dataset = get_loaders(\n",
    "    train_csv_path=config.DATASET + \"/train.csv\", test_csv_path=config.DATASET + \"/test.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/deepanshu/Desktop/ERAV1/S13/lightning_version/Final_trained_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_classes \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m YOLOv3(num_classes\u001b[39m=\u001b[39mnum_classes)\n\u001b[0;32m----> 3\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m/home/deepanshu/Desktop/ERAV1/S13/lightning_version/Final_trained_model.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/deepanshu/Desktop/ERAV1/S13/lightning_version/Final_trained_model.pth'"
     ]
    }
   ],
   "source": [
    "num_classes = 20\n",
    "model = YOLOv3(num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(\"/home/deepanshu/Desktop/ERAV1/S13/lightning_version/Final_trained_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfrered\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "print(\"Transfrered\")\n",
    "#import config\n",
    "#check_class_accuracy(model, test_loader, threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "def plot_image(image, boxes):\n",
    "    # Getting the color map from matplotlib\n",
    "    class_labels = [\n",
    "        \"aeroplane\",\n",
    "        \"bicycle\",\n",
    "        \"bird\",\n",
    "        \"boat\",\n",
    "        \"bottle\",\n",
    "        \"bus\",\n",
    "        \"car\",\n",
    "        \"cat\",\n",
    "        \"chair\",\n",
    "        \"cow\",\n",
    "        \"diningtable\",\n",
    "        \"dog\",\n",
    "        \"horse\",\n",
    "        \"motorbike\",\n",
    "        \"person\",\n",
    "        \"pottedplant\",\n",
    "        \"sheep\",\n",
    "        \"sofa\",\n",
    "        \"train\",\n",
    "        \"tvmonitor\"\n",
    "    ]\n",
    "    colour_map = plt.get_cmap(\"tab20b\")\n",
    "    # Getting 20 different colors from the color map for 20 different classes\n",
    "    colors = [colour_map(i) for i in np.linspace(0, 1, len(class_labels))]\n",
    "\n",
    "    # Reading the image with OpenCV\n",
    "    img = np.array(image)\n",
    "    # Getting the height and width of the image\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Add image to plot\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Plotting the bounding boxes and labels over the image\n",
    "    for box in boxes:\n",
    "        # Get the class from the box\n",
    "        class_pred = box[0]\n",
    "        # Get the center x and y coordinates\n",
    "        box = box[2:]\n",
    "        # Get the upper left corner coordinates\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "\n",
    "        # Create a Rectangle patch with the bounding box\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * w, upper_left_y * h),\n",
    "            box[2] * w,\n",
    "            box[3] * h,\n",
    "            linewidth=2,\n",
    "            edgecolor=colors[int(class_pred)],\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        \n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add class name to the patch\n",
    "        plt.text(\n",
    "            upper_left_x * w,\n",
    "            upper_left_y * h,\n",
    "            s=class_labels[int(class_pred)],\n",
    "            color=\"white\",\n",
    "            verticalalignment=\"top\",\n",
    "            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
    "        )\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def inference(model, test_loader):\n",
    "    count = 0\n",
    "    k = 30\n",
    "    #   anchors = config.ANCHORS\n",
    "    anchors = config.ANCHORS\n",
    "        \n",
    "    misclf = list()\n",
    "    classes = config.PASCAL_CLASSES\n",
    "    os.makedirs(\"missclassified_images/\", exist_ok=True)\n",
    "    # while count<=10:    \n",
    "    model.eval()\n",
    "    x,labels = test_loader.dataset[1]\n",
    "    x = x.to(\"cpu\")\n",
    "    predictions = model(x.unsqueeze(0))   \n",
    "    bboxes = [[] for _ in range(x.shape[0])]\n",
    "\n",
    "    batch_size = x.shape[0]\n",
    "    bboxes = [[] for _ in range(batch_size)]\n",
    "    for i in range(3):\n",
    "        S = predictions[i].shape[2]\n",
    "        anchor = torch.tensor([*anchors[i]]).to(\"cpu\") * S\n",
    "        boxes_scale_i = cells_to_bboxes(\n",
    "            predictions[i], anchor, S=S, is_preds=True\n",
    "        )\n",
    "        for idx, (box) in enumerate(boxes_scale_i):\n",
    "            bboxes[idx] += box\n",
    "    \n",
    "    nms_boxes = non_max_suppression(bboxes[idx], iou_threshold=0.4, threshold=0.4)\n",
    "    plot_image(x.permute(1,2,0).detach().cpu(), nms_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m std \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m inference(model, test_loader)\n",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m, in \u001b[0;36minference\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[39mfor\u001b[39;00m idx, (box) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(boxes_scale_i):\n\u001b[1;32m     27\u001b[0m         bboxes[idx] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m box\n\u001b[0;32m---> 29\u001b[0m nms_boxes \u001b[39m=\u001b[39m non_max_suppression(bboxes[idx], iou_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.4\u001b[39;49m, threshold\u001b[39m=\u001b[39;49m\u001b[39m0.4\u001b[39;49m)\n\u001b[1;32m     30\u001b[0m plot_image(x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu(), nms_boxes)\n",
      "File \u001b[0;32m~/Desktop/ERAV1/session13/lightning_version/custom_models/YOLOv3/utils.py:127\u001b[0m, in \u001b[0;36mnon_max_suppression\u001b[0;34m(bboxes, iou_threshold, threshold, box_format)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mwhile\u001b[39;00m bboxes:\n\u001b[1;32m    125\u001b[0m     chosen_box \u001b[39m=\u001b[39m bboxes\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m     bboxes \u001b[39m=\u001b[39m [\n\u001b[1;32m    128\u001b[0m         box\n\u001b[1;32m    129\u001b[0m         \u001b[39mfor\u001b[39;00m box \u001b[39min\u001b[39;00m bboxes\n\u001b[1;32m    130\u001b[0m         \u001b[39mif\u001b[39;00m box[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m chosen_box[\u001b[39m0\u001b[39m]\n\u001b[1;32m    131\u001b[0m         \u001b[39mor\u001b[39;00m intersection_over_union(\n\u001b[1;32m    132\u001b[0m             torch\u001b[39m.\u001b[39mtensor(chosen_box[\u001b[39m2\u001b[39m:]),\n\u001b[1;32m    133\u001b[0m             torch\u001b[39m.\u001b[39mtensor(box[\u001b[39m2\u001b[39m:]),\n\u001b[1;32m    134\u001b[0m             box_format\u001b[39m=\u001b[39mbox_format,\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m         \u001b[39m<\u001b[39m iou_threshold\n\u001b[1;32m    137\u001b[0m     ]\n\u001b[1;32m    139\u001b[0m     bboxes_after_nms\u001b[39m.\u001b[39mappend(chosen_box)\n\u001b[1;32m    141\u001b[0m \u001b[39mreturn\u001b[39;00m bboxes_after_nms\n",
      "File \u001b[0;32m~/Desktop/ERAV1/session13/lightning_version/custom_models/YOLOv3/utils.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mwhile\u001b[39;00m bboxes:\n\u001b[1;32m    125\u001b[0m     chosen_box \u001b[39m=\u001b[39m bboxes\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n\u001b[1;32m    127\u001b[0m     bboxes \u001b[39m=\u001b[39m [\n\u001b[1;32m    128\u001b[0m         box\n\u001b[1;32m    129\u001b[0m         \u001b[39mfor\u001b[39;00m box \u001b[39min\u001b[39;00m bboxes\n\u001b[1;32m    130\u001b[0m         \u001b[39mif\u001b[39;00m box[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m chosen_box[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 131\u001b[0m         \u001b[39mor\u001b[39;00m intersection_over_union(\n\u001b[1;32m    132\u001b[0m             torch\u001b[39m.\u001b[39;49mtensor(chosen_box[\u001b[39m2\u001b[39;49m:]),\n\u001b[1;32m    133\u001b[0m             torch\u001b[39m.\u001b[39;49mtensor(box[\u001b[39m2\u001b[39;49m:]),\n\u001b[1;32m    134\u001b[0m             box_format\u001b[39m=\u001b[39;49mbox_format,\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m         \u001b[39m<\u001b[39m iou_threshold\n\u001b[1;32m    137\u001b[0m     ]\n\u001b[1;32m    139\u001b[0m     bboxes_after_nms\u001b[39m.\u001b[39mappend(chosen_box)\n\u001b[1;32m    141\u001b[0m \u001b[39mreturn\u001b[39;00m bboxes_after_nms\n",
      "File \u001b[0;32m~/Desktop/ERAV1/session13/lightning_version/custom_models/YOLOv3/utils.py:89\u001b[0m, in \u001b[0;36mintersection_over_union\u001b[0;34m(boxes_preds, boxes_labels, box_format)\u001b[0m\n\u001b[1;32m     86\u001b[0m     box2_y2 \u001b[39m=\u001b[39m boxes_labels[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m3\u001b[39m:\u001b[39m4\u001b[39m]\n\u001b[1;32m     88\u001b[0m x1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(box1_x1, box2_x1)\n\u001b[0;32m---> 89\u001b[0m y1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmax(box1_y1, box2_y1)\n\u001b[1;32m     90\u001b[0m x2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmin(box1_x2, box2_x2)\n\u001b[1;32m     91\u001b[0m y2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmin(box1_y2, box2_y2)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "grid_size =(5,5)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [1,1,1]\n",
    "device = \"cpu\"\n",
    "inference(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/deepanshu/Desktop/ERAV1/S13/PASCAL_VOC/images/000001.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     plot_image(input_img\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu(), nms_boxes)\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m---> 32\u001b[0m Trueinference(model,Image\u001b[39m.\u001b[39;49mopen(\u001b[39m\"\u001b[39;49m\u001b[39m/home/deepanshu/Desktop/ERAV1/S13/PASCAL_VOC/images/000001.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/PIL/Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3215\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3217\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3218\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3219\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3221\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/deepanshu/Desktop/ERAV1/S13/PASCAL_VOC/images/000001.jpg'"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import cv2\n",
    "import os\n",
    "def Trueinference(model, input_img):\n",
    "    anchors = config.ANCHORS\n",
    "    transform_norm = transforms.Compose([transforms.ToTensor(), \n",
    "    transforms.Resize((416,416))])\n",
    "    # get normalized image\n",
    "    input_img = transform_norm(input_img).float()\n",
    "\n",
    "    model.eval()\n",
    "    predictions = model(input_img.unsqueeze(0))   \n",
    "    #input_img = input_img.permute(1,2,0)\n",
    "\n",
    "    bboxes = [[] for _ in range(input_img.shape[0])]\n",
    "\n",
    "    batch_size = input_img.shape[0]\n",
    "    bboxes = [[] for _ in range(batch_size)]\n",
    "    for i in range(3):\n",
    "        S = predictions[i].shape[2]\n",
    "        anchor = torch.tensor([*anchors[i]]).to(\"cpu\") * S\n",
    "        boxes_scale_i = cells_to_bboxes(\n",
    "            predictions[i], anchor, S=S, is_preds=True\n",
    "        )\n",
    "        for idx, (box) in enumerate(boxes_scale_i):\n",
    "            bboxes[idx] += box\n",
    "    \n",
    "    nms_boxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.5)\n",
    "    plot_image(input_img.permute(1,2,0).detach().cpu(), nms_boxes)\n",
    "\n",
    "from PIL import Image\n",
    "Trueinference(model,Image.open(\"/home/deepanshu/Desktop/ERAV1/S13/PASCAL_VOC/images/000001.jpg\").convert('RGB'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
