{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anant\\Downloads\\S22\\S22\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Optional, Literal, Any\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from lightning.fabric.strategies import FSDPStrategy\n",
    "from lightning.fabric.plugins import BitsandbytesPrecision\n",
    "\n",
    "\n",
    "from tsai_gpt.utils import get_default_supported_precision, gptq_quantization, load_checkpoint\n",
    "from tsai_gpt.model import GPT, Block, Config\n",
    "from tsai_gpt.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_num_samples_1(probs: torch.Tensor) -> torch.Tensor:\n",
    "    if torch._dynamo.is_compiling():\n",
    "        # Faster alternative to `torch.multinomial(probs, num_samples=1)` that is also CUDAGraph friendly\n",
    "        distribution = torch.empty_like(probs).exponential_(1)\n",
    "        return torch.argmax(probs / distribution, dim=-1, keepdim=True)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\n",
    "def sample(logits: torch.Tensor, temperature: float = 1.0, top_k: Optional[int] = None) -> torch.Tensor:\n",
    "    logits = logits[0, -1]\n",
    "    # optionally crop the logits to only the top k options\n",
    "    if top_k is not None:\n",
    "        v, i = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "        # do not use `torch.where` as in nanogpt because it will repeat top-k collisions\n",
    "        logits = torch.full_like(logits, float(\"-inf\")).scatter_(-1, i, v)\n",
    "    # optionally scale the logits and sample from a probability distribution\n",
    "    if temperature > 0.0:\n",
    "        probs = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
    "        return multinomial_num_samples_1(probs)\n",
    "    return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def next_token(model: GPT, input_pos: torch.Tensor, x: torch.Tensor, **kwargs: Any) -> torch.Tensor:\n",
    "    logits = model(x, input_pos)\n",
    "    next = sample(logits, **kwargs)\n",
    "    return next.type_as(x)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    model: GPT,\n",
    "    prompt: torch.Tensor,\n",
    "    max_returned_tokens: int,\n",
    "    *,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    eos_id: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n",
    "\n",
    "    The implementation of this function is modified from A. Karpathy's nanoGPT.\n",
    "\n",
    "    Args:\n",
    "        model: The model to use.\n",
    "        prompt: Tensor of shape (T) with indices of the prompt sequence.\n",
    "        max_returned_tokens: The maximum number of tokens to return (given plus generated).\n",
    "        temperature: Scales the predicted logits by 1 / temperature.\n",
    "        top_k: If specified, only sample among the tokens with the k highest probabilities.\n",
    "        eos_id: If specified, stop generating any more token once the <eos> token is triggered.\n",
    "    \"\"\"\n",
    "    T = prompt.size(0)\n",
    "    assert max_returned_tokens > T\n",
    "    if model.max_seq_length < max_returned_tokens - 1:\n",
    "        # rolling the kv cache based on the `input_pos` value would be necessary. However, doing so would introduce a\n",
    "        # data dependency on the `input_pos` tensor and impact model compilation. Since this setting is uncommon, we do\n",
    "        # not support it to avoid negatively impacting the overall speed\n",
    "        raise NotImplementedError(f\"max_seq_length {model.max_seq_length} needs to be >= {max_returned_tokens - 1}\")\n",
    "\n",
    "    device = prompt.device\n",
    "    tokens = [prompt]\n",
    "    input_pos = torch.tensor([T], device=device)\n",
    "    token = next_token(\n",
    "        model, torch.arange(0, T, device=device), prompt.view(1, -1), temperature=temperature, top_k=top_k\n",
    "    ).clone()\n",
    "    tokens.append(token)\n",
    "    for _ in range(2, max_returned_tokens - T + 1):\n",
    "        token = next_token(model, input_pos, token.view(1, -1), temperature=temperature, top_k=top_k).clone()\n",
    "        tokens.append(token)\n",
    "        if token == eos_id:\n",
    "            break\n",
    "        input_pos = input_pos.add_(1)\n",
    "    return torch.cat(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from saved_model/last-iter-015000-ckpt.pth\n",
      "Time to instantiate model: 0.70 seconds.\n",
      "Time to load the model weights: 0.88 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "quantize (Optional[Literal[&quot;bnb.nf4&quot;, &quot;bnb.nf4, optional): quantization method to use. Defaults to None.\n",
    "    - \"bnb.nf4\", \"bnb.nf4-dq\", \"bnb.fp4\", \"bnb.fp4-dq\": 4-bit quantization bitsandbytes\n",
    "    - \"bnb.int8\": 8-bit quantization bitsandbytes\n",
    "    - \"gptq.int4\": 4-bit quantization GPTQ\n",
    "    for more details see: https://github.com/facebookresearch/bitsandbytes, https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/quantize.md\n",
    "strategy (str, optional): Fabric strategy setting. Defaults to \"auto\".\n",
    "devices (int, optional): number of devices to be used. Defaults to 1.\n",
    "precision (Optional[str], optional): fabic precision settings. Defaults to None.\n",
    "\"\"\"\n",
    "\n",
    "chptk_path: str = \"saved_model/last-iter-015000-ckpt.pth\"\n",
    "tokenizer_path: str = \"tokenizer_Llama-2-7b-chat-hf\" \n",
    "quantize: Optional[Literal[\"bnb.nf4\", \"bnb.nf4-dq\", \"bnb.fp4\", \"bnb.fp4-dq\", \"bnb.int8\", \"gptq.int4\"]] = None\n",
    "strategy: str = \"auto\"\n",
    "devices: int = 1\n",
    "precision: Optional[str] = None\n",
    "\n",
    "precision = precision or get_default_supported_precision(training=False)\n",
    "plugins = None\n",
    "if quantize is not None:\n",
    "    if devices > 1:\n",
    "        raise NotImplemented(\"Multi-GPU quantization is not supported yet.\") \n",
    "    if quantize.startswith(\"bnb.\"):\n",
    "        if \"mixed\" in precision:\n",
    "            raise ValueError(\"Quantization and mixed precision is not supported.\")\n",
    "        dtype = {\"16-true\": torch.float16, \"bf16-true\": torch.bfloat16, \"32-true\": torch.float32}[precision]\n",
    "        plugins = BitsandbytesPrecision(quantize[4:], dtype)\n",
    "        precision = None\n",
    "\n",
    "if strategy==\"fsdp\":\n",
    "    strategy = FSDPStrategy(auto_wrap_policy={Block}, cpu_offload=False)\n",
    "\n",
    "fabric = L.Fabric(devices=devices, strategy=strategy, precision=precision, plugins=plugins)\n",
    "fabric.launch()\n",
    "\n",
    "tokenizer = Tokenizer(Path('tokenizer_Llama-2-7b-chat-hf'))\n",
    "config = Config.from_name(\"pythia-160m\")\n",
    "\n",
    "fabric.print(f\"Loading model from {chptk_path}\" , file=sys.stderr)\n",
    "t0 = time.perf_counter()\n",
    "with fabric.init_module(empty_init=True), gptq_quantization(quantize==\"gptq.int4\"):  \n",
    "    model = GPT(config)\n",
    "fabric.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\", file=sys.stderr)\n",
    "with fabric.init_tensor():\n",
    "    # enable the kv cache\n",
    "    model.set_kv_cache(batch_size=1)\n",
    "    \n",
    "model.eval()\n",
    "model = fabric.setup_module(model)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "load_checkpoint(fabric, model, chptk_path)\n",
    "fabric.print(f\"Time to load the model weights: {time.perf_counter() - t0:.02f} seconds.\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_prompt(\n",
    "    prompt: str = \"\",\n",
    "    *,\n",
    "    num_samples: int = 1,\n",
    "    max_new_tokens: int = 500,\n",
    "    top_k: int = 200,\n",
    "    temperature: float = 0.8,\n",
    "):\n",
    "    \"\"\"Generate text from a prompt using pre-trained model\n",
    "\n",
    "    Args:\n",
    "        prompt (str, optional): Prompt string to be used for generating samples. Defaults to \"\".\n",
    "        num_samples (int, optional): Number of samples to be generated. Defaults to 1.\n",
    "        max_new_tokens (int, optional): number of generation steps to take. Defaults to 500.\n",
    "        top_k (int, optional): top most preferable tokens to consider in the sampling process. Defaults to 200.\n",
    "        temperature (float, optional): Control randomness for sampelling process. Defaults to 0.8.\n",
    "    \"\"\"    \n",
    "    encoded = tokenizer.encode(prompt, device=fabric.device)\n",
    "    prompt_length = encoded.size(0)\n",
    "    max_returned_tokens = prompt_length + max_new_tokens\n",
    "    with fabric.init_tensor():\n",
    "        # set the max_seq_length to limit the memory usage to what we need\n",
    "        model.max_seq_length = max_returned_tokens\n",
    "        \n",
    "    for i in range(num_samples):\n",
    "        t0 = time.perf_counter()\n",
    "        y = generate(model, encoded, max_returned_tokens, temperature=temperature, top_k=top_k)\n",
    "        t = time.perf_counter() - t0\n",
    "        # for block in model.transformer.h:\n",
    "        #     block.attn.kv_cache.reset_parameters()\n",
    "        fabric.print(tokenizer.decode(y))\n",
    "        tokens_generated = y.size(0) - prompt_length\n",
    "        fabric.print(\n",
    "            f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr\n",
    "        )\n",
    "    if fabric.device.type == \"cuda\":\n",
    "        fabric.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\", file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a galaxy far, far away, an intergalactic council convenes to discuss the rising cost of lightsaber batteries. Among them is an unlikely representative: a droid with a penchant for economics...\n",
      "The sunlight, like a mountain of lithium, is the largest stream. The sky is the same type of storm: one of the stars in the mountain stars is a crucial stream you head to the sky. It was the largest speed of space for stars in the world. The sunlight is then the middle of a rayon. Here is the moonlight on the Earth of summer.\n",
      "And it is the same type of sky as the sunlight turns out to be the sunseted away. The sky will only be that of the world.\n",
      "To be sure, viewers are constantly talking about the cold weather and the sunlight, and they're beginning to see the cold weather.\n",
      "So, you see the sunlight on the moonlight and it reaches the sunlight. There is a massive perimeter, and you look across the sky. The sunlight makes you warm up on the moonlight.\n",
      "What the way the sun will be is, the sunlight drops off the Earth, the sunlight as it goes on the moonlight. Everybody is a planet along the ocean, the sunlight is the moonlight on the moonlight. The sunlight is the moonlight on earth and it is the moonlight on the horizon. The sunlight is twined with a year, the sunset over the moonlight to the sunlight.\n",
      "There is not an earthy wind on earth as the moonlight leaves the sunlight in the distant sky.\n",
      "Are you looking at this greenest Earth universe?\n",
      "One of the greatest source forests to ever understand the moonlight, but when one of the sun reaches sunlight. There is a few starters out there, but the sky is absolutely snowing.\n",
      "Is there a sunlight and a dark, like a sunlight on the moonlight.\n",
      "The sunlight is a small sunset of ice. It boasts the sky, winds a small lake and provides a beautiful atmosphere.\n",
      "The sun is the sky, the moonlight is coming, and the wind will see the sunlight on the sun, the moonlight. It is a wind, the moonlight and wind.\n",
      "The sunlight is just across the moonlight as the sun is sunlight.\n",
      "The sunlight is sunlight on the sun, the sunlight and the sunsets on the Earth. Once in the sunlight is the sun, water is a tuckle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 1: 26.72 sec total, 18.71 tokens/sec\n",
      "Memory used: 0.79 GB\n"
     ]
    }
   ],
   "source": [
    "generate_from_prompt(\n",
    "    prompt=\"In a galaxy far, far away, an intergalactic council convenes to discuss the rising cost of lightsaber batteries. Among them is an unlikely representative: a droid with a penchant for economics...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a galaxy far, far away, an intergalactic council convenes to discuss the rising cost of lightsaber batteries. Among them is an unlikely representative: a droid with a penchant for economics...\n",
      "Very few people have been able to say that the Earth's energy costs are a more expensive, uncommon, and uncommon.\n",
      "A major change of time, the electrical infrastructure, the electrical infrastructure, and the electrical infrastructure, is a growing threat to the planet.\n",
      "A strong, sharp, and uncommon sense of power, the electrical infrastructure is a fact that the electrical infrastructure is a major part of the Earth's energy plan.\n",
      "The scientists agree that the electrical infrastructure is a part of the Earth's energy project, a direct-engineered solution to a more efficient, efficient and efficient solution.\n",
      "The project is a major concern: a new, uncommon task force at the Earth's energy costs is to make it easier for the electricity of the solar energy sector to produce a solar energy infrastructure.\n",
      "The solar energy infrastructure is a major concern: a project of 1000 solar energy costs is an important part of the solar energy sector in the world.\n",
      "The energy sector is a major factor that is sustainable and is a major concern for the energy generation.\n",
      "The project is a project of 1.233 billion solar energy costs, 370.1 billion renewables are a major asset, and 1.4 percent are a global energy resources.\n",
      "The project is a 1.4.3 billion.3 billion solar energy costs are a significant increase of 10.4 billion solar energy costs are a significant factor.\n",
      "The project is a 5.3.9 billion solar energy costs are a significant factor in the energy sector.\n",
      "The project is a huge contribution to the energy sector in power generation.\n",
      "The project is a national energy infrastructure and the world's energy infrastructure.\n",
      "The project is a 1.3 billion renewable energy infrastructure in the world.\n",
      "The project is a 1.3 billion solar energy infrastructure, a 2.3 billion solar energy infrastructure, a 12 billion solar energy infrastructure, and the largest solar energy infrastructure.\n",
      "The project's energy infrastructure is a key priority of the project.\n",
      "The project is a 2.3.\n",
      "The project is a project of 10.\n",
      "The project is aims to provide energy infrastructure and energy storage solutions to the solar energy costs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 1: 28.01 sec total, 17.85 tokens/sec\n",
      "Memory used: 0.79 GB\n"
     ]
    }
   ],
   "source": [
    "generate_from_prompt(\n",
    "    prompt=\"In a galaxy far, far away, an intergalactic council convenes to discuss the rising cost of lightsaber batteries. Among them is an unlikely representative: a droid with a penchant for economics...\",\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Sherlock Holmes and Dr. Watson enter the world of social media influencers, they find their first case: the mysterious disappearance of a famous TikTok star's like button. At the point of this, a former victim of a former tad, and what the GOP is looking for, I'm not sure what it's happening, but I'm trying to bring a big fight to the world of social media influencers. They are everywhere, but with these two-time friends and spending a good time. They seem to be a 'competent' star to have such diverse channels as the GOP 'makeup' of a single killer.\n",
      "Still, thank you for posting this series on this show, and be a penn by the way. This is one of the most amazing things in the world. The role of TikTok is a national music festival, the national television community, the media and the world.\n",
      "We describe how TikTok is the best song on TikTok.\n",
      "This year, we were a long time, and the band was part of a television show that brought the audience to the right place. Once we began to explore the show, the band takes on the community of TikTok (the band), and each of the band members will be part of a concert. We were our favorite kids and they had a chance to dance, and bring their music and listen to their music.\n",
      "We also have a band together.\n",
      "Our voice is a talented, and very generous, and a lottopper in our community. We really enjoyed our work, we were working on a lot in the way. We are always involved, and it is our most exciting and doing. We're proud to be part of We want to have the future of TikTok.\n",
      "When we heard the band last night we'd love to learn about the music, and we wanted to listen to music. We listened to music in our country, and I learned that's really cool. The group, we've the music, all over the world, but we go to groups like who actually connect songs and sing this song. We've always enjoyed a lot of the songwriting and we have more listeners than we have the music we've used.\n",
      "There were four days in the \"Gloodies\" band, and each 1 and we are just the same to what we've expected. We have to make music in the greatest way that we are. We were the most exc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 1: 6.48 sec total, 77.18 tokens/sec\n",
      "Memory used: 0.79 GB\n"
     ]
    }
   ],
   "source": [
    "generate_from_prompt(\n",
    "    prompt=\"As Sherlock Holmes and Dr. Watson enter the world of social media influencers, they find their first case: the mysterious disappearance of a famous TikTok star's like button.\",\n",
    "    \n",
    "    # temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Sherlock Holmes and Dr. Watson enter the world of social media influencers, they find their first case: the mysterious disappearance of a famous TikTok star's like button. So, it's a humiliating and challenging story of the ties, the people who hilariously make the world the way they live. That's why we've found that the camera still does.\n",
      "(D) In the background of the 2008 novel, the reality show becomes the story of a fictional story of the TikTok star's novel. The ties of the TikTok event are an extremely vulnerable, but it's important to remember that the story of a younger man's imagination is not just a way to understand the story of his own.\n",
      "It's about the ties between the TikTok star and the world.\n",
      "While we'll have to make the story of the TikTok star's story, we'll throw in the world of the horror and the world.\n",
      "The TikTok star is the greatest hero. Despite his story of the tragedy, it's worth a lot of fun to find an out story.\n",
      "To see, the TikTok star is a fictional story. While there is no way to get a story in ties to the story.\n",
      "It'sely isn't the worst fight in the world. But the way that's happened is just a challenge to the audience.\n",
      "(D) This movie tells the story of a stranger who'd been in theaters, and who's been in the eyes of a girlfriend's.\n",
      "(D) But if you're in a weird way, it's just a dull.\n",
      "(D) The TikTok star is a pretty good movie. And no, it's a good movie.\n",
      "(D) There was a lot of talk about the TikTok star. I think we'll be talking to the girlfriend, who is so famous to the story of theater in theater and why she's actually beaten after the film in the 190s. But, after all the stories that get shaped by the way I think TikTok had been.\n",
      "(D) There's later, I was watching the movie like, I had plenty of moments.\n",
      "The TikTok star started to hit in theater, and I had a breakout. She's pretty much smiling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 1: 6.26 sec total, 79.84 tokens/sec\n",
      "Memory used: 0.79 GB\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
