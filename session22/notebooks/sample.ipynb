{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI-vIFfvqWTm",
        "outputId": "84ab0d2a-5d27-407d-b9fa-c3c35a57f778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anant\\Downloads\\S22\\S22\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bpur3YB7qWTr"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Optional, Literal, Any\n",
        "\n",
        "import lightning as L\n",
        "import torch\n",
        "from lightning.fabric.strategies import FSDPStrategy\n",
        "from lightning.fabric.plugins import BitsandbytesPrecision\n",
        "\n",
        "\n",
        "from tsai_gpt.utils import get_default_supported_precision, gptq_quantization, load_checkpoint\n",
        "from tsai_gpt.model import GPT, Block, Config\n",
        "from tsai_gpt.tokenizer import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEe1LDQ8qWTs",
        "outputId": "6aace895-1afe-49a4-aa5d-a9af58300600"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Seed set to 1234\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1234\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1234"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "L.seed_everything(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s5fTOKW9qWTt"
      },
      "outputs": [],
      "source": [
        "def multinomial_num_samples_1(probs: torch.Tensor) -> torch.Tensor:\n",
        "    if torch._dynamo.is_compiling():\n",
        "        # Faster alternative to `torch.multinomial(probs, num_samples=1)` that is also CUDAGraph friendly\n",
        "        distribution = torch.empty_like(probs).exponential_(1)\n",
        "        return torch.argmax(probs / distribution, dim=-1, keepdim=True)\n",
        "    return torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "\n",
        "def sample(logits: torch.Tensor, temperature: float = 1.0, top_k: Optional[int] = None) -> torch.Tensor:\n",
        "    logits = logits[0, -1]\n",
        "    # optionally crop the logits to only the top k options\n",
        "    if top_k is not None:\n",
        "        v, i = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "        # do not use `torch.where` as in nanogpt because it will repeat top-k collisions\n",
        "        logits = torch.full_like(logits, float(\"-inf\")).scatter_(-1, i, v)\n",
        "    # optionally scale the logits and sample from a probability distribution\n",
        "    if temperature > 0.0:\n",
        "        probs = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "        return multinomial_num_samples_1(probs)\n",
        "    return torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "def next_token(model: GPT, input_pos: torch.Tensor, x: torch.Tensor, **kwargs: Any) -> torch.Tensor:\n",
        "    logits = model(x, input_pos)\n",
        "    next = sample(logits, **kwargs)\n",
        "    return next.type_as(x)\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate(\n",
        "    model: GPT,\n",
        "    prompt: torch.Tensor,\n",
        "    max_returned_tokens: int,\n",
        "    *,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: Optional[int] = None,\n",
        "    eos_id: Optional[int] = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n",
        "\n",
        "    The implementation of this function is modified from A. Karpathy's nanoGPT.\n",
        "\n",
        "    Args:\n",
        "        model: The model to use.\n",
        "        prompt: Tensor of shape (T) with indices of the prompt sequence.\n",
        "        max_returned_tokens: The maximum number of tokens to return (given plus generated).\n",
        "        temperature: Scales the predicted logits by 1 / temperature.\n",
        "        top_k: If specified, only sample among the tokens with the k highest probabilities.\n",
        "        eos_id: If specified, stop generating any more token once the <eos> token is triggered.\n",
        "    \"\"\"\n",
        "    T = prompt.size(0)\n",
        "    assert max_returned_tokens > T\n",
        "    if model.max_seq_length < max_returned_tokens - 1:\n",
        "        # rolling the kv cache based on the `input_pos` value would be necessary. However, doing so would introduce a\n",
        "        # data dependency on the `input_pos` tensor and impact model compilation. Since this setting is uncommon, we do\n",
        "        # not support it to avoid negatively impacting the overall speed\n",
        "        raise NotImplementedError(f\"max_seq_length {model.max_seq_length} needs to be >= {max_returned_tokens - 1}\")\n",
        "\n",
        "    device = prompt.device\n",
        "    tokens = [prompt]\n",
        "    input_pos = torch.tensor([T], device=device)\n",
        "    token = next_token(\n",
        "        model, torch.arange(0, T, device=device), prompt.view(1, -1), temperature=temperature, top_k=top_k\n",
        "    ).clone()\n",
        "    tokens.append(token)\n",
        "    for _ in range(2, max_returned_tokens - T + 1):\n",
        "        token = next_token(model, input_pos, token.view(1, -1), temperature=temperature, top_k=top_k).clone()\n",
        "        tokens.append(token)\n",
        "        if token == eos_id:\n",
        "            break\n",
        "        input_pos = input_pos.add_(1)\n",
        "    return torch.cat(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ7jrUXAqWTv",
        "outputId": "5b907928-9a56-4637-b871-5e6714420c07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading model from /content/drive/MyDrive/last-iter-015000-ckpt.pth\n",
            "Time to instantiate model: 0.03 seconds.\n",
            "Time to load the model weights: 26.28 seconds.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "quantize (Optional[Literal[&quot;bnb.nf4&quot;, &quot;bnb.nf4, optional): quantization method to use. Defaults to None.\n",
        "    - \"bnb.nf4\", \"bnb.nf4-dq\", \"bnb.fp4\", \"bnb.fp4-dq\": 4-bit quantization bitsandbytes\n",
        "    - \"bnb.int8\": 8-bit quantization bitsandbytes\n",
        "    - \"gptq.int4\": 4-bit quantization GPTQ\n",
        "    for more details see: https://github.com/facebookresearch/bitsandbytes, https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/quantize.md\n",
        "strategy (str, optional): Fabric strategy setting. Defaults to \"auto\".\n",
        "devices (int, optional): number of devices to be used. Defaults to 1.\n",
        "precision (Optional[str], optional): fabic precision settings. Defaults to None.\n",
        "\"\"\"\n",
        "\n",
        "chptk_path: str = \"last-iter-015000-ckpt.pth\"\n",
        "tokenizer_path: str = \"tokenizer_Llama-2-7b-chat-hf\"\n",
        "quantize: Optional[Literal[\"bnb.nf4\", \"bnb.nf4-dq\", \"bnb.fp4\", \"bnb.fp4-dq\", \"bnb.int8\", \"gptq.int4\"]] = None\n",
        "strategy: str = \"auto\"\n",
        "devices: int = 1\n",
        "precision: Optional[str] = None\n",
        "\n",
        "precision = precision or get_default_supported_precision(training=False)\n",
        "plugins = None\n",
        "if quantize is not None:\n",
        "    if devices > 1:\n",
        "        raise NotImplemented(\"Multi-GPU quantization is not supported yet.\")\n",
        "    if quantize.startswith(\"bnb.\"):\n",
        "        if \"mixed\" in precision:\n",
        "            raise ValueError(\"Quantization and mixed precision is not supported.\")\n",
        "        dtype = {\"16-true\": torch.float16, \"bf16-true\": torch.bfloat16, \"32-true\": torch.float32}[precision]\n",
        "        plugins = BitsandbytesPrecision(quantize[4:], dtype)\n",
        "        precision = None\n",
        "\n",
        "if strategy==\"fsdp\":\n",
        "    strategy = FSDPStrategy(auto_wrap_policy={Block}, cpu_offload=False)\n",
        "\n",
        "fabric = L.Fabric(devices=devices, strategy=strategy, precision=precision, plugins=plugins)\n",
        "fabric.launch()\n",
        "\n",
        "tokenizer = Tokenizer(Path('tokenizer_Llama-2-7b-chat-hf'))\n",
        "config = Config.from_name(\"pythia-160m\")\n",
        "\n",
        "fabric.print(f\"Loading model from {chptk_path}\" , file=sys.stderr)\n",
        "t0 = time.perf_counter()\n",
        "with fabric.init_module(empty_init=True), gptq_quantization(quantize==\"gptq.int4\"):\n",
        "    model = GPT(config)\n",
        "fabric.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\", file=sys.stderr)\n",
        "with fabric.init_tensor():\n",
        "    # enable the kv cache\n",
        "    model.set_kv_cache(batch_size=1)\n",
        "\n",
        "model.eval()\n",
        "model = fabric.setup_module(model)\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "load_checkpoint(fabric, model, chptk_path)\n",
        "fabric.print(f\"Time to load the model weights: {time.perf_counter() - t0:.02f} seconds.\", file=sys.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "r7AN7Lo3qWTx"
      },
      "outputs": [],
      "source": [
        "def generate_from_prompt(\n",
        "    prompt: str = \"\",\n",
        "    *,\n",
        "    num_samples: int = 1,\n",
        "    max_new_tokens: int = 500,\n",
        "    top_k: int = 200,\n",
        "    temperature: float = 0.8,\n",
        "):\n",
        "    \"\"\"Generate text from a prompt using pre-trained model\n",
        "\n",
        "    Args:\n",
        "        prompt (str, optional): Prompt string to be used for generating samples. Defaults to \"\".\n",
        "        num_samples (int, optional): Number of samples to be generated. Defaults to 1.\n",
        "        max_new_tokens (int, optional): number of generation steps to take. Defaults to 500.\n",
        "        top_k (int, optional): top most preferable tokens to consider in the sampling process. Defaults to 200.\n",
        "        temperature (float, optional): Control randomness for sampelling process. Defaults to 0.8.\n",
        "    \"\"\"\n",
        "    encoded = tokenizer.encode(prompt, device=fabric.device)\n",
        "    prompt_length = encoded.size(0)\n",
        "    max_returned_tokens = prompt_length + max_new_tokens\n",
        "    with fabric.init_tensor():\n",
        "        # set the max_seq_length to limit the memory usage to what we need\n",
        "        model.max_seq_length = max_returned_tokens\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        t0 = time.perf_counter()\n",
        "        y = generate(model, encoded, max_returned_tokens, temperature=temperature, top_k=top_k)\n",
        "        t = time.perf_counter() - t0\n",
        "        # for block in model.transformer.h:\n",
        "        #     block.attn.kv_cache.reset_parameters()\n",
        "        fabric.print(tokenizer.decode(y))\n",
        "        tokens_generated = y.size(0) - prompt_length\n",
        "        fabric.print(\n",
        "            f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr\n",
        "        )\n",
        "    if fabric.device.type == \"cuda\":\n",
        "        fabric.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\", file=sys.stderr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_Q-EgIoqWTx",
        "outputId": "bc6f8c81-8c7e-431c-a592-6412c2a0dfc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In a galaxy far, far away, an intergalactic council convenes to discuss the rising cost of lightsaber batteries. Among them is an unlikely representative: a droid with a penchant for economics...\n",
            "https://avinpeace.com/canon/business-advertime-from-to-top-or-to-frivional-rescue-of-girls.html\n",
            "Why aren’t they worsened to keep things alive?\n",
            "Human rights activists are going to provide a healthy, unilateral, universal understanding of the environment and what it means to them. They are very much in the sense of the environment. They need to be able to understand and analyze the environment and there is something in the world that has benefited from the world.\n",
            "This is one of the least important, most important concepts: it is the world that a healthy, poor and more complicated social order is how the environment is developed. This is not only the most common cause of destruction, but it is the reality that it is the world that has its best. It is the way that we see the end of the world, and the way we see it.\n",
            "https://avinpeace.com/v-guan/\n",
            "A special plan is to ensure that we see these changes. It is not only necessary to help us understand the future as we move our experiences and future. We provide a safe, unlucky means to this emerging.\n",
            "https://avinpeace.com/canon/business-induces-to-american-class\n",
            "This event is not only a matter of life but a function as it relates to the world. It is usually a state that is over 1000 people’s of life. There is no point of difference between life and life, it is the way we do. We have a healthy, honest, positive, and funerial environment for every person, at a time of life, and to life.\n",
            "https://avinomysmia.com/canon/they/all-is-is-work-the-ge-the-time-to-you\n",
            "https://avinpeace.com/canon/by/williamobomys\n",
            "https://avinpeace.com/canon/canon/about-invest/\n",
            "https://avinwerravecare.com/about/vjay/weartypedefefefefefefefefefefefefefefefefefefefefefef\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Time for inference 1: 124.59 sec total, 4.01 tokens/sec\n"
          ]
        }
      ],
      "source": [
        "generate_from_prompt(\n",
        "    prompt=\"In a galaxy far, far away, an intergalactic council convenes to discuss the rising cost of lightsaber batteries. Among them is an unlikely representative: a droid with a penchant for economics...\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8_LwowDqWTy",
        "outputId": "5807ad6c-4b38-47bc-81e4-fb8804c9c832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Two roads diverged in a yellow wood,\n",
            "            Robert Frost poetAnd sorry I could not travel both\n",
            "            And be one traveler, long I stood\n",
            "            And looked down one as far as I could\n",
            "            To where it bent in the undergrowth;\n",
            "\n",
            "            Then took the other, as just as fair,\n",
            "            And having perhaps the better claim,\n",
            "            Because it was grassy and wanted wear;\n",
            "            Though as for that the passing there\n",
            "            Had worn them really about the same,...\n",
            "            But when the rest of the day was the right place, it was the first time that the rest of the day had been gone,\n",
            "            And so I said,\n",
            "            And the last time I had seen the rest of the day, I was not sure how to get the rest of the day\n",
            "            And was it the best place to go, and to go, and to get the rest of the day, and to get the rest of the day, and to get the rest of the day, and to get the rest of the day, and to get the rest of the day, and to get the rest of the day, and to get the rest of the day, and to get the rest of the day, and to get the rest of the day, and to get the rest of you will be able to get the rest on the day, and to get the rest of the day, and to get the rest of the day, and then have the rest of the day, and then get the rest of the day, and then take the rest of the day, and then, and then and then to get the rest of the day, and then take the rest of the day, to get the rest of the day, and then go again to get the rest of day, and then take the rest of the night, and then we will bring the rest of the day, and then leave the rest of day, and then leave the rest of day we will take the rest to get the end of day, and then and again, and then, and then we will leave the rest of the day, and then, and then go, and then we will take the next day, and then we will, and then again, and then again, and then again we will not get the rest of the day, but we will start to get the rest of the day, and then and then, and then we will again, and again, again, and then, then we will again, then and then we will we will not have time to go, and then we will take the time to the next day, and then we will not be able to make the rest of the day, but then we will we will again, and then, and then, we will not have time to wait for the day to get the rest of the day, and then, and then we will go, but then we will not go and then, and\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Time for inference 1: 129.69 sec total, 3.86 tokens/sec\n"
          ]
        }
      ],
      "source": [
        "generate_from_prompt(\n",
        "    prompt=\"\"\"Two roads diverged in a yellow wood,\n",
        "            Robert Frost poetAnd sorry I could not travel both\n",
        "            And be one traveler, long I stood\n",
        "            And looked down one as far as I could\n",
        "            To where it bent in the undergrowth;\n",
        "\n",
        "            Then took the other, as just as fair,\n",
        "            And having perhaps the better claim,\n",
        "            Because it was grassy and wanted wear;\n",
        "            Though as for that the passing there\n",
        "            Had worn them really about the same,...\"\"\",\n",
        "    temperature=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iRcBRePqWTy",
        "outputId": "6f318ba4-87fd-4468-ce90-7319e431dc27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After receiving an encrypted message from a sinister organisation, with the help of Madeleine, James Bond sets out to uncover a conspiracy, which reveals an ugly truth about his past..\n",
            "For his friends, the love of one whose actions were used in both the speeches and the news media. The truth is that this is the only one where every person of the real person who is in a peaceful movement. Of those who were in fact truly unlocked for the pleasure of him, the truth of his efforts is that it is not a matter of reason for people to accept their freedom. He was a priest of notorious spirituality and injustice for all of his teachings in the community than before he did.\n",
            "However the same was true. His reputation for freedom and his desire for truth be in the heart of the world.\n",
            "On behalf of the world, the world has established itself as a symbol of freedom.\n",
            "Another life has raised a life of free speech, an unprecedented nature in art and culture.\n",
            "For this purpose, in this case, the world has been condemned by the nature of the internet and the experience of the world. The world has been resurrected by the political revolution and in its modern world, and so the environment.\n",
            "The world has gone to become a natural world of free speech, the world offers some sense of freedom and the world.\n",
            "In order to end the world, the world can live today and live again.\n",
            "The world has grown and managed to raise the courage.\n",
            "Another way of life is it, and a lot of love that is nothing in this world, is not the way to do it. It is always a very bad thing for their power, and their power.\n",
            "They build a country.\n",
            "The only people like your own in this world may try to make their love.\n",
            "But what we do we do?\n",
            "It is a reason to be prosperous enough for the better.\n",
            "For the world or the world—there is no sense, or no reason to be able to freeze.\n",
            "Without the truth we have the power that we can achieve, and all the power of the world does not. But like many one other, it is not only places.\n",
            "What does this mean to be that?\n",
            "Let us be a world of free and freedom that is not an education.\n",
            "But what is we want to take care of, and respect my faith and respect.\n",
            "Is it is a pleasure to be a better and respect that will in the future be built for the people, and for those who are\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Time for inference 1: 116.33 sec total, 4.30 tokens/sec\n"
          ]
        }
      ],
      "source": [
        "generate_from_prompt(\n",
        "    prompt=\"After receiving an encrypted message from a sinister organisation, with the help of Madeleine, James Bond sets out to uncover a conspiracy, which reveals an ugly truth about his past..\",\n",
        "\n",
        "    # temperature=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiJzBT5TqWTz",
        "outputId": "de32f764-0322-4dd7-ef96-8d2a882fb5d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Out of no where yesterday night I saw a dream, I was walking on a lonely road alone, listening music throughout and ..\n",
            "\"A spy troll down a little my little guy I'm a freakin' haasan. We have a lot of people here because I'm looking at every single person on the internet and I didn't know anyone who's going to do it for him anymore. We have a lot of people around the world who know it. I'm a fan of the music industry in the industry. I'm very fortunate to have a lot of people here and there.\"\n",
            "What do you think about this? I'm not thinking about it. I think it's the biggest thing that I imagined in terms of the marketing aspect. You know, I think it's the largest problem that I've seen after the third and I think people are talking about it. I think they're going to go there and I mean that's what the way it's going to be like in a way that's going to be like in a way that's going to be like \"a good thing.\"\n",
            "But the best thing I think about this was that I was like \"you're not winning the music in the first year\". In that's going to be like \"whoever you think\" and \"what's uproot disrupting the kids\" about how they really don't want to be like \"what's the hysteria.\"\n",
            "So it's not really the same thing. I think that's really the next thing. I've seen a lot of people. I think that've gotten through a lot of the kind of bust, but that's happening right. I've got some help out there and I know people are there most of the best things I've seen. I haven's been in touching my band in this year, and so I'm very long. \"I've been kind of a lot of that have been great kids, with the big music and I'm going to talk about you guys.\"\n",
            "\"I think it've got a lot of people here for a couple of those years, but I think there's an audience that we have. There's in the past are an important topic to get there. We are all on top of that and that's all the way out of ours, I think.\"\n",
            "The next weekend\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Time for inference 1: 116.23 sec total, 4.30 tokens/sec\n"
          ]
        }
      ],
      "source": [
        "generate_from_prompt(\n",
        "    prompt=\"Out of no where yesterday night I saw a dream, I was walking on a lonely road alone, listening music throughout and ..\",\n",
        "\n",
        "    # temperature=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXUtzdyP4jzJ",
        "outputId": "11be4325-e37c-4c78-c40d-0bdc7839957a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A wall with beautiful paintings lot of colors, arts, theams and many more, experiecing such thing is a gift in itself ..\n",
            "The Cooligolo is a proud place in which he is now serving with his wife the Queen. If you’re interested in developing this new adventure, please call us today for more information.\n",
            "We are pleased to receive updates or comments that keep your home in the future, and that do not mean we are going to stay safe and enjoy. RF1: The Difference Between A Millionaire and the Stousand Yard\n",
            "In total, Yard’s 25th-century career began in 1990. A total of 59,997 people were included in today’s national food supply of food. But at least 25,000 people received their annual life of life. Mothers’ 1929 retail outreach was the only country that was built in 1991. The following year, she reserves the power of the great deal to serve 8,000 people over the next 25 years. She has long been in the military as well as internationally as international volunteers. She completed her three meetings with the World Heritage Foundation.\n",
            "She has served as a member of the Council and the Council of the Council, which she is a member of the UNAAS Women’s Association.\n",
            "With over 5 years of business and working in the construction industry, she has been awarded the pastoral and tragedies of the pastoral rights and the development of the North American Worker Company. She has served as a partner and manager of the Committee of the American Airlines Trust and has also served as the top supplier in the industry. She has served as a her fellow of the North America’s Conspiracy, in the 190’s and 1190’s, as well as the NBA for her husband and wife. Lana DID is a full-time staff writer and consultant on the 19th Annual Cathyaticanes Center, serving as President of Dr.\n",
            "This is a group of 715,0000 people. For more than 6.5 million people have lived in about 6000 in the market.\n",
            "Late an annual conference organized by the New York Times that said the Yard and her family lived in 5666679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Time for inference 1: 115.86 sec total, 4.32 tokens/sec\n"
          ]
        }
      ],
      "source": [
        "generate_from_prompt(\n",
        "    prompt=\"A wall with beautiful paintings lot of colors, arts, theams and many more, experiecing such thing is a gift in itself ..\",\n",
        "\n",
        "    # temperature=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWrN8RuU6EUe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
